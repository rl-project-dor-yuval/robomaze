{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f637bf72-3b31-4453-ade5-8fb6a43900f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e0c999-c5da-4a12-a308-f2b47124a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 25 2021 20:16:47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MazeEnv as mz\n",
    "from MazeEnv import Rewards\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DDPG\n",
    "import Evaluation\n",
    "from Evaluation import EvalAndSaveCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0276fc2f-203b-4c2a-b808-c06fed15c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circular_map(size, radius):\n",
    "    center = np.divide(size, 2)\n",
    "    x, y = np.ogrid[:size[0], :size[1]]\n",
    "    maze_map = np.where(np.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2) > radius, 1, 0)\n",
    "\n",
    "    return maze_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b911f33-bd5a-43dd-aea8-219262d90020",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_LOC = (5, 2.8)\n",
    "TIMEOUT_STEPS = 200\n",
    "BUFFER_SIZE = 1000 # smaller buffer for small task\n",
    "TOTAL_TIME_STEPS = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "REWARDS = Rewards(target_arrival=1, collision=-1, timeout=-0.5)\n",
    "\n",
    "EVAL_EPISODES=5\n",
    "EVAL_FREQ=500\n",
    "VIDEO_FREQ=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b80c36f-2579-42f8-9a81-a29b12bfaf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# create environment :\n",
    "tile_size = 0.1\n",
    "maze_size = mz.MazeSize.SQUARE10\n",
    "map_size = np.dot(maze_size, int(1 / tile_size))\n",
    "maze_map = make_circular_map(map_size, 3 / tile_size)\n",
    "# maze_map = np.zeros(map_size)\n",
    "\n",
    "maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                              maze_map=maze_map,\n",
    "                              tile_size=tile_size,\n",
    "                              start_loc=START_LOC,\n",
    "                              target_loc=np.divide(maze_size, 2),\n",
    "                              timeout_steps=TIMEOUT_STEPS,\n",
    "                              show_gui=False,\n",
    "                              rewards=REWARDS), \n",
    "                  filename=\"logs/DummyMaze/results\")\n",
    "_ = maze_env.reset()\n",
    "\n",
    "check_env(maze_env)\n",
    "\n",
    "# create separete evaluation environment:\n",
    "eval_maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                                   maze_map=maze_map,\n",
    "                                   tile_size=tile_size,\n",
    "                                   start_loc=START_LOC,\n",
    "                                   target_loc=np.divide(maze_size, 2),\n",
    "                                   timeout_steps=TIMEOUT_STEPS,\n",
    "                                   show_gui=False,\n",
    "                                   rewards=REWARDS)\n",
    "                       )\n",
    "_ = eval_maze_env.reset()\n",
    " \n",
    "# create model:\n",
    "model = DDPG(policy=\"MlpPolicy\",\n",
    "             env=maze_env,\n",
    "             buffer_size=BUFFER_SIZE,\n",
    "             learning_rate=LEARNING_RATE,\n",
    "             device='cuda',\n",
    "             train_freq=(1, \"episode\"),\n",
    "             verbose=1)\n",
    "\n",
    "# create callback for evaluation\n",
    "callback = EvalAndSaveCallback(log_dir=\"logs/DummyMaze\",\n",
    "                               eval_env=eval_maze_env,\n",
    "                               eval_freq=EVAL_FREQ,\n",
    "                               eval_episodes=EVAL_EPISODES,\n",
    "                               eval_video_freq=VIDEO_FREQ,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb18c11-8585-4953-9caa-e73847cf6d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIME_STEPS,\n",
    "            callback=callback,)\n",
    "\n",
    "print(\"time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cffbb1-9619-494a-b964-cb03a903b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation.plot_train_eval_results(\"logs/DummyMaze\", EVAL_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f06ad-2599-4543-bafd-c31e5079f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import ipyplot\n",
    "import glob\n",
    "\n",
    "# load best model \n",
    "best_model = DDPG.load(\"./logs/DummyMaze/best_model\")\n",
    "\n",
    "Evaluation.plot_train_eval_results(\"logs/DummyMaze\", EVAL_EPISODES)\n",
    "episode_reward = Evaluation.record_model(best_model, eval_maze_env, \"logs/DummyMaze/final.avi\")\n",
    "print(\"Reward:\", episode_reward)\n",
    "\n",
    "Evaluation.create_gifs_from_avi(\"logs/DummyMaze\")\n",
    "gifs = glob.glob(\"logs/DummyMaze/*.gif\")\n",
    "labels = [pth.split('/')[-1].split('.')[0] for pth in gifs]\n",
    "ipyplot.plot_images(gifs, labels, img_width=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0cb63-310d-4aa7-8d7a-76f905c0cbbb",
   "metadata": {},
   "source": [
    "# Gradually trainning\n",
    "\n",
    "Up until here, it only worked to train the robot to get to the target when it starts really close to it. When taking it a little bit farther we would need a lot of luck to make it learn.\n",
    "\n",
    "Here we are trying to train, and each *m* steps change the envrionment so the robot starts a little bit further from the target and the circle radius is a little bit bigger, and it has a little bit more steps so it can learn gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd1bee91-9e7a-489b-88c9-b15533782a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start locations:\n",
      " [(5, 3.3), (5, 3.2), (5, 3.1), (5, 3.0), (5, 2.9), (5, 2.8), (5, 2.7), (5, 2.6), (5, 2.5), (5, 2.4)]\n",
      "Timeout Steps:\n",
      " [150. 170. 190. 210. 230. 250. 270. 290. 310. 330.]\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 1000 # smaller buffer for small task\n",
    "# TOTAL_TIME_STEPS = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "REWARDS = Rewards(target_arrival=1, collision=-1, timeout=-0.5)\n",
    "\n",
    "EVAL_EPISODES=5\n",
    "EVAL_FREQ=200\n",
    "VIDEO_FREQ=5\n",
    "\n",
    "CHANGE_ENV_EVERY = 1000\n",
    "\n",
    "START_LOC_X = [5]*10\n",
    "START_LOC_Y = np.around(np.linspace(3.3, 2.4, 10), 1)\n",
    "START_LOCATIONS = list(zip(START_LOC_X, START_LOC_Y))\n",
    "print(\"Start locations:\\n\", START_LOCATIONS)\n",
    "# MAZE_RADIUSES = np.around(np.linspace(2.9, 3.8, 10), 1)\n",
    "# print(\"Maze Radiuses:\\n\", MAZE_RADIUSES)\n",
    "TIMEOUT_STEPS = np.linspace(150, 330, 10)\n",
    "print(\"Timeout Steps:\\n\", TIMEOUT_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751ec67-d33f-4c99-ad36-5143ece2bf72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HER - Hindsight Experience Replay\n",
    "\n",
    "After succeed in training the ant from relatively close place to the target (this is thanks to random success), we can observe that after moving the ant a little bit further from the target it is pretty hard to learn a long sequence of actions that will lead to goal arrival randomly.\n",
    "We could optionally set the reward as a function of the distance to the target, but doing that will harm our reward sparsity, and make the reward much dense. We would like to achieve the same target with sparse reward (success or failure), for this mission we can use HER.\n",
    "By using HER, we actually learn from unsuccessful episodes. The key insight that HER formalizes is what humans do intuitively: Even though we have not succeeded at a specific goal, we have at least achieved a different one.\n",
    "When the RL algorithm (DDPG in our case) is wrong, we pretend that we wanted to achieve this goal (in our case, a different target location) if we repeat this process, we will eventually learn how to achieve arbitrary goals, including the goals that we really want to achieve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc857c20-50eb-4445-902d-ab8ebc07dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import HerReplayBuffer\n",
    "\n",
    "START_LOC = (5, 2.8)\n",
    "TIMEOUT_STEPS = 300\n",
    "BUFFER_SIZE = 1000 # smaller buffer for small task\n",
    "TOTAL_TIME_STEPS = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "REWARDS = Rewards(target_arrival=1, collision=-1, timeout=0)\n",
    "\n",
    "EVAL_EPISODES=1\n",
    "EVAL_FREQ=1000\n",
    "VIDEO_FREQ=2\n",
    "\n",
    "#HER parameters\n",
    "N_SAMPLED = 4\n",
    "STRATEGY = 'future' # futute, random or episode\n",
    "ONLINE_SAMPLING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d3543f-ab70-4950-a79d-10d04e1bd183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "argv[0]=\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# create environment :\n",
    "tile_size = 0.1\n",
    "maze_size = mz.MazeSize.SQUARE10\n",
    "map_size = np.dot(maze_size, int(1 / tile_size))\n",
    "maze_map = make_circular_map(map_size, 4 / tile_size)\n",
    "# maze_map = np.zeros(map_size)\n",
    "\n",
    "maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                              maze_map=maze_map,\n",
    "                              tile_size=tile_size,\n",
    "                              start_loc=START_LOC,\n",
    "                              target_loc=np.divide(maze_size, 2),\n",
    "                              timeout_steps=TIMEOUT_STEPS,\n",
    "                              show_gui=False,\n",
    "                              rewards=REWARDS), \n",
    "                  filename=\"logs/DummyMaze/results\")\n",
    "_ = maze_env.reset()\n",
    "check_env(maze_env)\n",
    "\n",
    "# create separete evaluation environment:\n",
    "eval_maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                                   maze_map=maze_map,\n",
    "                                   tile_size=tile_size,\n",
    "                                   start_loc=START_LOC,\n",
    "                                   target_loc=np.divide(maze_size, 2),\n",
    "                                   timeout_steps=TIMEOUT_STEPS,\n",
    "                                   show_gui=False,\n",
    "                                   rewards=REWARDS)\n",
    "                       )\n",
    "_ = eval_maze_env.reset()\n",
    " \n",
    "# create model:\n",
    "model = DDPG(policy=\"MultiInputPolicy\",\n",
    "             env=maze_env,\n",
    "             buffer_size=BUFFER_SIZE,\n",
    "             learning_rate=LEARNING_RATE,\n",
    "             device='cuda',\n",
    "             train_freq=(1, \"episode\"),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(\n",
    "                 n_sampled_goal=N_SAMPLED,\n",
    "                 goal_selection_strategy=STRATEGY,\n",
    "                 online_sampling=ONLINE_SAMPLING,\n",
    "                 max_episode_length=TIMEOUT_STEPS,\n",
    "             ),\n",
    "             verbose=1)\n",
    "\n",
    "# create callback for evaluation\n",
    "callback = EvalAndSaveCallback(log_dir=\"logs/DummyMaze\",\n",
    "                               eval_env=eval_maze_env,\n",
    "                               eval_freq=EVAL_FREQ,\n",
    "                               eval_episodes=EVAL_EPISODES,\n",
    "                               eval_video_freq=VIDEO_FREQ,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b9864-c22e-41f7-8bbb-8b2fa47227eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Steps evaluation, avg reward:-1.0, avg episode length: 288.0\n",
      "--Saving new best model--\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -0.25    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 264      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total timesteps | 1188     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.235    |\n",
      "|    critic_loss     | 0.000472 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 900      |\n",
      "---------------------------------\n",
      "2000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "--Saving new best model--\n",
      "creating video\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 298      |\n",
      "|    ep_rew_mean     | -0.125   |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total timesteps | 2388     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.08     |\n",
      "|    critic_loss     | 0.000596 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2088     |\n",
      "---------------------------------\n",
      "3000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | -0.0833  |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total timesteps | 3588     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.999    |\n",
      "|    critic_loss     | 0.000286 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3288     |\n",
      "---------------------------------\n",
      "4000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "creating video\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | -0.0625  |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total timesteps | 4788     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.933    |\n",
      "|    critic_loss     | 0.000242 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4488     |\n",
      "---------------------------------\n",
      "5000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | -0.05    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 111      |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total timesteps | 5988     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.874    |\n",
      "|    critic_loss     | 0.000188 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5688     |\n",
      "---------------------------------\n",
      "6000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "creating video\n",
      "7000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | -0.0417  |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total timesteps | 7188     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.821    |\n",
      "|    critic_loss     | 0.000228 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6888     |\n",
      "---------------------------------\n",
      "8000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "creating video\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | -0.0357  |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total timesteps | 8388     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.77     |\n",
      "|    critic_loss     | 0.000162 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8088     |\n",
      "---------------------------------\n",
      "9000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | -0.0312  |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total timesteps | 9588     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.723    |\n",
      "|    critic_loss     | 0.000126 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9288     |\n",
      "---------------------------------\n",
      "10000 Steps evaluation, avg reward:0.0, avg episode length: 300.0\n",
      "creating video\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#clean all movies from the previous run\n",
    "!rm -f logs/DummyMaze/*_steps.avi\n",
    "\n",
    "torch.manual_seed(3295)\n",
    "model.learn(total_timesteps=TOTAL_TIME_STEPS,\n",
    "            callback=callback)\n",
    " \n",
    "print(\"time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522a241-c9c0-460c-aaee-b8554580a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import ipyplot\n",
    "import glob\n",
    "\n",
    "# load best model \n",
    "best_model = DDPG.load(\"./logs/DummyMaze/best_model\", env=maze_env)\n",
    "\n",
    "Evaluation.plot_train_eval_results(\"logs/DummyMaze\", EVAL_EPISODES)\n",
    "episode_reward = Evaluation.record_model(best_model, eval_maze_env, \"logs/DummyMaze/final.avi\")\n",
    "print(\"Reward:\", episode_reward)\n",
    "\n",
    "#Clean previous gifs\n",
    "!rm -f logs/DummyMaze/*_steps.gif\n",
    "\n",
    "# Plot gifs\n",
    "Evaluation.create_gifs_from_avi(\"logs/DummyMaze\")\n",
    "gifs = glob.glob(\"logs/DummyMaze/*.gif\")\n",
    "labels = [pth.split('/')[-1].split('.')[0] for pth in gifs]\n",
    "ipyplot.plot_images(gifs, labels, img_width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244c13e-ccc4-455c-ad5c-7b1e961efd12",
   "metadata": {},
   "source": [
    "### Re-evaluate the model on harder case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6215b2-0db3-455e-a365-cd5cf0c93e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "\n",
    "# load best model \n",
    "best_model = DDPG.load(\"./logs/DummyMaze/best_model\", env=maze_env)\n",
    "\n",
    "# Define another  evaluation environment:\n",
    "\n",
    "START_LOC = (5, 3)\n",
    "TIMEOUT_STEPS = 500\n",
    "REWARDS = Rewards(target_arrival=1, collision=-1, timeout=0)\n",
    "EVAL_EPISODES = 1\n",
    "\n",
    "tile_size = 0.1\n",
    "maze_size = mz.MazeSize.SQUARE10\n",
    "map_size = np.dot(maze_size, int(1 / tile_size))\n",
    "maze_map = make_circular_map(map_size, 3 / tile_size)\n",
    "\n",
    "fin_eval_maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                                   maze_map=maze_map,\n",
    "                                   tile_size=tile_size,\n",
    "                                   start_loc=START_LOC,\n",
    "                                   target_loc=np.divide(maze_size, 2),\n",
    "                                   timeout_steps=TIMEOUT_STEPS,\n",
    "                                   show_gui=False,\n",
    "                                   rewards=REWARDS)\n",
    "                       )\n",
    "_ = fin_eval_maze_env.reset()\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "rewards, lengths = evaluate_policy(best_model, fin_eval_maze_env, EVAL_EPISODES,\n",
    "                                   deterministic=True, return_episode_rewards=True)\n",
    "\n",
    "#create _ video\n",
    "video_path = os.path.join(\"logs/DummyMaze/fin_eval\", f'eval_steps.avi')\n",
    "obs = fin_eval_maze_env.reset(create_video=True, video_path=video_path)\n",
    "while True:\n",
    "    action, _ = best_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = fin_eval_maze_env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print(\"Reward:\", rewards)\n",
    "# #Clean previous gifs\n",
    "os.makedirs(\"logs/DummyMaze/fin_eval\", exist_ok=True)\n",
    "!rm -f logs/DummyMaze/fin_eval/*_steps.gif\n",
    "\n",
    "# # Plot gifs\n",
    "Evaluation.create_gifs_from_avi(\"logs/DummyMaze/fin_eval\")\n",
    "gifs = glob.glob(\"logs/DummyMaze/fin_eval/*.gif\")\n",
    "labels = [pth.split('/')[-1].split('.')[0] for pth in gifs]\n",
    "ipyplot.plot_images(gifs, labels, img_width=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c84c10-5b64-4a87-939a-41253953747a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
