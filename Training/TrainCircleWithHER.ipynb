{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cc2d56-0135-4f26-9070-2912108e84c3",
   "metadata": {},
   "source": [
    "## HER - Hindsight Experience Replay\n",
    "\n",
    "After succeed in training the ant from relatively close place to the target (this is thanks to random success), we can observe that after moving the ant a little bit further from the target it is pretty hard to learn a long sequence of actions that will lead to goal arrival randomly.\n",
    "We could optionally set the reward as a function of the distance to the target, but doing that will harm our reward sparsity, and make the reward much dense. We would like to achieve the same target with sparse reward (success or failure), for this mission we can use HER.\n",
    "By using HER, we actually learn from unsuccessful episodes. The key insight that HER formalizes is what humans do intuitively: Even though we have not succeeded at a specific goal, we have at least achieved a different one.\n",
    "When the RL algorithm (DDPG in our case) is wrong, we pretend that we wanted to achieve this goal (in our case, a different target location) if we repeat this process, we will eventually learn how to achieve arbitrary goals, including the goals that we really want to achieve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4d9c2-7dbd-4c22-b48f-0c7eb39a8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import HerReplayBuffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0252a31e-09c9-40ac-bc9d-57e4db547ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b39936d-7adc-43a4-a384-6e191055e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import HerReplayBuffer\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import MazeEnv.MazeEnv as mz\n",
    "from MazeEnv.MazeEnv import Rewards\n",
    "from MazeEnv.Utils import *\n",
    "from Evaluation import EvalAndSaveCallback\n",
    "import Evaluation\n",
    "\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d63eb4-97bf-495e-af96-c7b5e4dc7d1e",
   "metadata": {},
   "source": [
    "<h1> Remiender: batch size is important! </h1>\n",
    "<h1> Remiender: batch size is important! </h1>\n",
    "<h1> Remiender: batch size is important! </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9abc17b-6083-4b09-83a2-1ccd6b1a9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_LOC = (5, 2.8)\n",
    "TIMEOUT_STEPS = 300\n",
    "BUFFER_SIZE = 1000 # smaller buffer for small task\n",
    "TOTAL_TIME_STEPS = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "REWARDS = Rewards(target_arrival=1, collision=-1, timeout=0)\n",
    "\n",
    "EVAL_EPISODES=1\n",
    "EVAL_FREQ=1000\n",
    "VIDEO_FREQ=2\n",
    "\n",
    "\n",
    "#HER parameters\n",
    "N_SAMPLED = 4\n",
    "STRATEGY = 'future' # futute, random or episode\n",
    "ONLINE_SAMPLING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a7ca747a-045f-4e99-8724-e62ffd4e435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# create environment :\n",
    "tile_size = 0.1\n",
    "maze_size = mz.MazeSize.SQUARE10\n",
    "map_size = np.dot(maze_size, int(1 / tile_size))\n",
    "maze_map = make_circular_map(map_size, 5 / tile_size)\n",
    "# maze_map = np.zeros(map_size)\n",
    "\n",
    "maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                              maze_map=maze_map,\n",
    "                              tile_size=tile_size,\n",
    "                              start_loc=START_LOC,\n",
    "                              target_loc=np.divide(maze_size, 2),\n",
    "                              timeout_steps=TIMEOUT_STEPS,\n",
    "                              show_gui=False,\n",
    "                              rewards=REWARDS), \n",
    "                  filename=\"logs/DummyMaze/results\")\n",
    "_ = maze_env.reset()\n",
    "\n",
    "# create separete evaluation environment:\n",
    "eval_maze_env = Monitor(mz.MazeEnv(maze_size=maze_size,\n",
    "                                   maze_map=maze_map,\n",
    "                                   tile_size=tile_size,\n",
    "                                   start_loc=START_LOC,\n",
    "                                   target_loc=np.divide(maze_size, 2),\n",
    "                                   timeout_steps=TIMEOUT_STEPS,\n",
    "                                   show_gui=False,\n",
    "                                   rewards=REWARDS)\n",
    "                       )\n",
    "_ = eval_maze_env.reset()\n",
    " \n",
    "# create model:\n",
    "model = DDPG(policy=\"MultiInputPolicy\",\n",
    "             env=maze_env,\n",
    "             buffer_size=BUFFER_SIZE,\n",
    "             learning_rate=LEARNING_RATE,\n",
    "             device='cuda',\n",
    "             train_freq=(1, \"episode\"),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(\n",
    "                 n_sampled_goal=N_SAMPLED,\n",
    "                 goal_selection_strategy=STRATEGY,\n",
    "                 online_sampling=ONLINE_SAMPLING,\n",
    "                 max_episode_length=TIMEOUT_STEPS,\n",
    "             ),\n",
    "             verbose=1)\n",
    "\n",
    "# create callback for evaluation\n",
    "callback = EvalAndSaveCallback(log_dir=\"logs/DummyMaze\",\n",
    "                               eval_env=eval_maze_env,\n",
    "                               eval_freq=EVAL_FREQ,\n",
    "                               eval_episodes=EVAL_EPISODES,\n",
    "                               eval_video_freq=VIDEO_FREQ,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9fed5bb-f86b-401b-b976-f8d1095ffa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51d0fe-8f54-4b26-a95c-eb238512ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#clean all movies from the previous run\n",
    "!rm -f logs/DummyMaze/*_steps.avi\n",
    "\n",
    "torch.manual_seed(3295)\n",
    "model.learn(total_timesteps=TOTAL_TIME_STEPS,\n",
    "            callback=callback)\n",
    " \n",
    "print(\"time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731de49-fc42-4548-85cd-df7e5eb4f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import ipyplot\n",
    "import glob\n",
    "\n",
    "# load best model \n",
    "best_model = DDPG.load(\"./logs/DummyMaze/best_model\", env=maze_env)\n",
    "\n",
    "Evaluation.plot_train_eval_results(\"logs/DummyMaze\", EVAL_EPISODES)\n",
    "episode_reward = Evaluation.record_model(best_model, eval_maze_env, \"logs/DummyMaze/final.avi\")\n",
    "print(\"Reward:\", episode_reward)\n",
    "\n",
    "#Clean previous gifs\n",
    "!rm -f logs/DummyMaze/*_steps.gif\n",
    "\n",
    "# Plot gifs\n",
    "Evaluation.create_gifs_from_avi(\"logs/DummyMaze\")\n",
    "gifs = glob.glob(\"logs/DummyMaze/*.gif\")\n",
    "labels = [pth.split('/')[-1].split('.')[0] for pth in gifs]\n",
    "ipyplot.plot_images(gifs, labels, img_width=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
