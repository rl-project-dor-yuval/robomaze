{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f637bf72-3b31-4453-ade5-8fb6a43900f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e0c999-c5da-4a12-a308-f2b47124a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import MazeEnv.MultiTargetMazeEnv as mtmz\n",
    "from MazeEnv.MazeEnv import Rewards\n",
    "from Utils import make_circular_map, clear_files, get_multi_targets_circle_envs\n",
    "from Evaluation import EvalAndSaveCallback, MultiTargetEvalAndSaveCallback\n",
    "import Evaluation\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8048c9-b6ca-460a-b2cb-f24162b57bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME=\"8m_mass_and_friction\"\n",
    "os.makedirs(os.path.join(\"logs/MultiTargets\", RUN_NAME), exist_ok=True)\n",
    "\n",
    "TOTAL_TIME_STEPS = 8_000_000 #number of training timesteps\n",
    "BUFFER_SIZE = 500_000\n",
    "TIMEOUT_STEPS = 1000 # Timeout Steps of each episode \n",
    "LEARNING_RATE = 2e-7\n",
    "REDUCE_LR = True\n",
    "EXPLORATION_NOISE_STD = 0.05\n",
    "LEARNING_STARTS=5000\n",
    "ACTION_FORCE=1500\n",
    "\n",
    "REWARDS = Rewards(target_arrival=10, collision=-20, timeout=-10, idle=-0.01, fall=-20)\n",
    "\n",
    "\n",
    "# EVAL_EPISODES = 30\n",
    "EVAL_FREQ = 50_000\n",
    "VIDEO_FREQ = 20\n",
    "\n",
    "targets = np.genfromtxt(\"TestTargets/test_coords.csv\", delimiter=',')\n",
    "\n",
    "maze_env, eval_maze_env = get_multi_targets_circle_envs(radius=2.9,\n",
    "                                                        targets = targets,\n",
    "                                                        timeout_steps=TIMEOUT_STEPS,\n",
    "                                                        rewards=REWARDS,\n",
    "                                                        monitor_dir=os.path.join(\"logs/MultiTargets\", RUN_NAME, \"results\"),\n",
    "                                                        action_force=ACTION_FORCE)\n",
    " \n",
    "# create model:\n",
    "exploration_noise = NormalActionNoise(mean=np.array([0]*8), sigma=np.array([EXPLORATION_NOISE_STD]*8))\n",
    "\n",
    "def lr_func(progress):\n",
    "    if progress < 0.5 and REDUCE_LR:\n",
    "        return LEARNING_RATE/5\n",
    "    return LEARNING_RATE\n",
    "\n",
    "set_random_seed(314, True)\n",
    "\n",
    "model = DDPG(policy=\"MlpPolicy\",\n",
    "             env=maze_env,\n",
    "             buffer_size=BUFFER_SIZE,\n",
    "             learning_rate=lr_func,\n",
    "             action_noise=exploration_noise,\n",
    "             device=device,\n",
    "             train_freq=(1, \"episode\"),\n",
    "             learning_starts=LEARNING_STARTS,\n",
    "             verbose=0,\n",
    "             tensorboard_log=\"./logs/MultiTargets/tb\")\n",
    "\n",
    "# create callback for evaluation\n",
    "callback = MultiTargetEvalAndSaveCallback(log_dir=os.path.join(\"logs/MultiTargets\", RUN_NAME),\n",
    "                                          eval_env=eval_maze_env,\n",
    "                                          eval_freq=EVAL_FREQ,\n",
    "                                          eval_video_freq=VIDEO_FREQ,\n",
    "                                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cffca0-1866-4ea2-a0eb-31704111d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old videos: \n",
    "clear_files('logs/MultiTargets/*.gif')\n",
    "clear_files('logs/MultiTargets/*.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb18c11-8585-4953-9caa-e73847cf6d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000.00 Steps evaluation, avg reward:-19.99, avg episode length: 1000.00, success rate: 0.00\n",
      "--Saving new best model--\n",
      "100000.00 Steps evaluation, avg reward:-19.99, avg episode length: 1000.00, success rate: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1710/3371114748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model.learn(total_timesteps=TOTAL_TIME_STEPS,\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             tb_log_name=RUN_NAME)\n",
      "\u001b[0;32m~/robomaze/venvremote/lib/python3.9/site-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    128\u001b[0m     ) -> OffPolicyAlgorithm:\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         return super(DDPG, self).learn(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/robomaze/venvremote/lib/python3.9/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    200\u001b[0m     ) -> OffPolicyAlgorithm:\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         return super(TD3, self).learn(\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/robomaze/venvremote/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;31m# do as many gradients steps as steps performed during the rollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mgradient_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/robomaze/venvremote/lib/python3.9/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Optimize the critics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/robomaze/venvremote/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIME_STEPS,\n",
    "            callback=callback,\n",
    "            tb_log_name=RUN_NAME)\n",
    " \n",
    "print(\"time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54eef51-d16a-46b3-b2a6-d70cb171c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation.plot_train_eval_results(\"logs/MultiTargets\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f06ad-2599-4543-bafd-c31e5079f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import ipyplot\n",
    "import glob\n",
    "\n",
    "# create gifs and plot them:\n",
    "Evaluation.create_gifs_from_avi(\"logs/MultiTargets\")\n",
    "\n",
    "gifs = glob.glob(\"logs/MultiTargets/*_steps.gif\")\n",
    "labels = [pth.split('/')[-1].split('.')[0] for pth in gifs]\n",
    "ipyplot.plot_images(gifs, labels, img_width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6555f15-eb07-4145-91ca-e34fb6e926bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "best_model = DDPG.load(\"./logs/MultiTargets/best_model\",env=maze_env)\n",
    "\n",
    "# evaluate for last final episode (random target)\n",
    "episode_reward = Evaluation.record_model(best_model, eval_maze_env, \"logs/MultiTargets/final/final.avi\")\n",
    "print(\"Final Evaluation Reward:\", episode_reward)\n",
    "\n",
    "Evaluation.create_gifs_from_avi(\"logs/MultiTargets/final\")\n",
    "\n",
    "gifs = glob.glob(\"logs/MultiTargets/final/final.gif\")\n",
    "print(gifs)\n",
    "ipyplot.plot_images(gifs, img_width=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fbfec-42d3-4fe7-ba9d-2362de8f1aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
